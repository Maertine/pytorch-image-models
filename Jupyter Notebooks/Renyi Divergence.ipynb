{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example values for the probabilities\n",
    "x = torch.tensor([[9/25,12/25,4/25],[2/7,1/4,1/5]],requires_grad=True)\n",
    "y = torch.tensor([[1/3,1/3,1/3],[1/2,5/4,1/5]])\n",
    "print(x)\n",
    "print(y)"
   ],
   "id": "5f572b6e4ab33904",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example of divergence being infinity\n",
    "#x = torch.tensor([[-0.7753, -0.7236, -0.6072, -0.8284, -0.8085, -0.8923, -0.6853, -0.8315,-0.8302, -0.7934]])\n",
    "#y = torch.tensor([[-9.7151e+00, -7.6585e+00, -7.3228e+00, -7.9473e-08, -1.0569e+01,-6.7095e+00, -1.1969e+01, -7.1102e+00, -5.1660e+00, -6.2902e+00]])\n",
    "#print(x)\n",
    "#print(y)"
   ],
   "id": "aff0480a2c3b2339",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Definition of Renyi Divergence with logits as input\n",
    "class RenyiDivergence(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(RenyiDivergence, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, input_logits, target_logits):\n",
    "        if self.alpha == 1.0: # KL Divergence\n",
    "            log_sums = torch.logsumexp(input_logits, dim=1)-torch.logsumexp(target_logits, dim=1)\n",
    "            \n",
    "            exp_target_logits = torch.exp(target_logits)\n",
    "            sum_target_exp = exp_target_logits.sum(dim=1, keepdim=True)\n",
    "            \n",
    "            loss = torch.sum((exp_target_logits / sum_target_exp) * (target_logits - input_logits + log_sums.unsqueeze(1)), dim=1)\n",
    "            \n",
    "            return loss.mean()\n",
    "        \n",
    "        target_exp = torch.exp(target_logits)\n",
    "        input_exp = torch.exp(input_logits)\n",
    "        \n",
    "        target_power = target_exp.pow(self.alpha)\n",
    "        input_power = input_exp.pow(1-self.alpha)\n",
    "        \n",
    "        target_sum_exp = torch.sum(target_exp,dim=1)\n",
    "        input_sum_exp = torch.sum(input_exp,dim=1)\n",
    "        \n",
    "        loss = 1/(self.alpha-1) * (torch.log(torch.sum(target_power*input_power,dim=1)) - self.alpha*torch.log(target_sum_exp) + (self.alpha-1)*torch.log(input_sum_exp))\n",
    "        \n",
    "        return loss.mean()"
   ],
   "id": "4130e895be638180",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test\n",
    "x = torch.tensor([[2/3,1/3,1/3],[2/3,1/3,1/3]],requires_grad=True)\n",
    "y = torch.tensor([[1/3,2/3,1/3],[1/3,2/3,1/3]])\n",
    "output = RenyiDivergence(alpha=1)(x,y)\n",
    "print(output)\n",
    "output.backward()\n",
    "print(x.grad)"
   ],
   "id": "f5085ca5437ce909",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Preparation for plot creation\n",
    "Q = torch.tensor([[1/3,2/3]])\n",
    "Q = torch.log(Q)\n",
    "lists = []\n",
    "for alpha in [0.5,1,2,10]:\n",
    "    list = []\n",
    "    for p in range(501):\n",
    "        P = torch.tensor([[p/500,1-p/500]])\n",
    "        P = torch.log(P)\n",
    "        divergence = RenyiDivergence(alpha=alpha)(Q,P)\n",
    "        list.append(divergence)\n",
    "    lists.append(list)\n",
    "    \n",
    "P = torch.tensor([[1/3,2/3]])\n",
    "P = torch.log(P)\n",
    "lists_2 = []\n",
    "for alpha in [0.5,1,2,10]:\n",
    "    list = []\n",
    "    for q in range(501):\n",
    "        Q = torch.tensor([[q/500,1-q/500]])\n",
    "        Q = torch.log(Q)\n",
    "        divergence = RenyiDivergence(alpha=alpha)(Q,P)\n",
    "        list.append(divergence)\n",
    "    lists_2.append(list)"
   ],
   "id": "f52a2eccdcf7ad68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Ploting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(np.array(range(501)) / 500, np.transpose(np.array(lists[0])), color='blue', linestyle='-', label=r'$\\alpha = 1/2$')\n",
    "ax.plot(np.array(range(501)) / 500, np.transpose(np.array(lists[1])), color='red', linestyle='--', label=r'$\\alpha = 1$')\n",
    "ax.plot(np.array(range(501)) / 500, np.transpose(np.array(lists[2])), color='orange', linestyle=':', label=r'$\\alpha = 2$')\n",
    "ax.plot(np.array(range(501)) / 500, np.transpose(np.array(lists[3])), color='green', linestyle='-.', label=r'$\\alpha = 10$')\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.annotate(r'$p$', xy=(1.05, 0), xytext=(10, -20), \n",
    "            textcoords='offset points', ha='center', va='center', fontsize=12)\n",
    "\n",
    "ax.annotate(r'$D_\\alpha(P \\| Q)$', xy=(0, 1.05), xytext=(-40, 10), \n",
    "            textcoords='offset points', ha='center', va='center', fontsize=12)\n",
    "\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "fig.savefig('Renyi_Divergence.png')\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "43d0d92a4b387087",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(np.array(range(501)) / 500, np.transpose(np.array(lists_2[0])), color='blue', linestyle='-', label=r'$\\alpha = 1/2$')\n",
    "ax.plot(np.array(range(501)) / 500, np.transpose(np.array(lists_2[1])), color='red', linestyle='--', label=r'$\\alpha = 1$')\n",
    "ax.plot(np.array(range(501)) / 500, np.transpose(np.array(lists_2[2])), color='orange', linestyle=':', label=r'$\\alpha = 2$')\n",
    "ax.plot(np.array(range(501)) / 500, np.transpose(np.array(lists_2[3])), color='green', linestyle='-.', label=r'$\\alpha = 10$')\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.annotate(r'$q$', xy=(1.05, 0), xytext=(10, -20), \n",
    "            textcoords='offset points', ha='center', va='center', fontsize=12)\n",
    "\n",
    "ax.annotate(r'$D_\\alpha(P \\| Q)$', xy=(0, 5.45), xytext=(-40, 10), \n",
    "            textcoords='offset points', ha='center', va='center', fontsize=12)\n",
    "\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "fig.savefig('Renyi_Divergence_2.png')\n",
    "\n",
    "plt.show()"
   ],
   "id": "f62fa07c2953c1b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define transforms for the training and test sets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [0.95,0.05], generator=torch.Generator().manual_seed(872))\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, generator=torch.Generator().manual_seed(872))\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True, generator=torch.Generator().manual_seed(872))\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ],
   "id": "16d43d390489c715",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TeacherNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TeacherNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 1024) \n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_input = 0.1\n",
    "        self.dropout_hidden = 0.2\n",
    "        self.is_training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the image\n",
    "        x = F.dropout(x, p=self.dropout_input, training=self.is_training)\n",
    "        x = F.dropout(F.relu(self.fc1(x)), p=self.dropout_hidden, training=self.is_training)\n",
    "        x = F.dropout(F.relu(self.fc2(x)), p=self.dropout_hidden, training=self.is_training)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "id": "a3332e4361c31eda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher_model = TeacherNetwork().to(device)\n",
    "optimizer = optim.SGD(teacher_model.parameters(), lr=0.05, momentum=0.5, weight_decay=3e-4)\n",
    "num_epochs = 10\n",
    "\n",
    "wandb.init(\n",
    "    project=\"Renyi_Divergence_MNIST\",\n",
    "    name = \"Teacher Model\",\n",
    "    config={}\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'---Epoch {epoch+1}---')\n",
    "    \n",
    "    train_loss, train_accuracy, size = 0, 0, 0\n",
    "    for i, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = teacher_model(data)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * targets.size(0)\n",
    "        train_accuracy += torch.sum(torch.argmax(outputs, dim=1) == targets).item()\n",
    "        size += targets.size(0)\n",
    "        \n",
    "    train_loss = train_loss/size\n",
    "    train_accuracy = train_accuracy/size\n",
    "    \n",
    "    print(f'Average Train Loss: {train_loss:.4f} \\t \\t Train Accuracy: {100*train_accuracy:.2f}%')\n",
    "    \n",
    "    teacher_model.is_training = False\n",
    "    \n",
    "    val_loss, val_accuracy, size = 0, 0, 0\n",
    "    for i, (data, targets) in enumerate(val_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = teacher_model(data)\n",
    "            val_loss += nn.CrossEntropyLoss()(pred, targets) * targets.shape[0]\n",
    "            val_accuracy += torch.sum(torch.argmax(pred, dim=1) == targets).item()\n",
    "        size += targets.shape[0]\n",
    "    \n",
    "    val_loss, val_accuracy = val_loss / size, val_accuracy / size\n",
    "    \n",
    "    print(f'Average Validation Loss: {val_loss:.4f} \\t Validation Accuracy: {100*val_accuracy:.2f}%')\n",
    "    \n",
    "    wandb.log({\"train_ce_loss\": train_loss,\n",
    "               \"val_ce_loss\": val_loss,\n",
    "               \"train_accuracy\": 100*train_accuracy,\n",
    "               \"val_accuracy\": 100*val_accuracy,\n",
    "               \"epoch\": epoch\n",
    "               }\n",
    "    )\n",
    "    \n",
    "    teacher_model.is_training = True\n",
    "\n",
    "teacher_model.is_training = False\n",
    "test_loss, test_accuracy, size = 0, 0, 0\n",
    "\n",
    "for i, (data, targets) in enumerate(test_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = teacher_model(data)\n",
    "            test_loss += nn.CrossEntropyLoss()(pred, targets) * targets.shape[0]\n",
    "            test_accuracy += torch.sum(torch.argmax(pred, dim=1) == targets).item()\n",
    "        size += targets.shape[0]\n",
    "\n",
    "test_loss, test_accuracy = test_loss / size, test_accuracy / size\n",
    "\n",
    "print(f'Training finished.')\n",
    "print(f'Average Validation Loss: {test_loss:.4f}\\t Validation Accuracy: {100*test_accuracy:.2f}%')\n",
    "\n",
    "wandb.finish()"
   ],
   "id": "8b8930e3139148e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class StudentNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        self.dropout_input = 0.1\n",
    "        self.dropout_hidden = 0.2\n",
    "        self.is_training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the image\n",
    "        x = F.dropout(x, p=self.dropout_input, training=self.is_training)\n",
    "        x = F.dropout(F.relu(self.fc1(x)), p=self.dropout_hidden, training=self.is_training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ],
   "id": "6729eee0dfb975d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vanilla_model = StudentNetwork().to(device)\n",
    "optimizer = optim.SGD(vanilla_model.parameters(), lr=0.01, momentum=0.5, weight_decay=5e-4)\n",
    "num_epochs = 10\n",
    "\n",
    "wandb.init(\n",
    "    project=\"Renyi_Divergence_MNIST\",\n",
    "    name = \"Vanilla Model\",\n",
    "    config={}\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'---Epoch {epoch+1}---')\n",
    "    \n",
    "    train_loss, train_accuracy, size = 0, 0, 0\n",
    "    for i, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vanilla_model(data)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * targets.size(0)\n",
    "        train_accuracy += torch.sum(torch.argmax(outputs, dim=1) == targets).item()\n",
    "        size += targets.size(0)\n",
    "        \n",
    "    train_loss = train_loss/size\n",
    "    train_accuracy = train_accuracy/size\n",
    "    \n",
    "    print(f'Average Train Loss: {train_loss:.4f} \\t \\t Train Accuracy: {100*train_accuracy:.2f}%')\n",
    "    \n",
    "    vanilla_model.is_training = False\n",
    "    \n",
    "    val_loss, val_accuracy, size = 0, 0, 0\n",
    "    for i, (data, targets) in enumerate(val_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = vanilla_model(data)\n",
    "            val_loss += nn.CrossEntropyLoss()(pred, targets) * targets.shape[0]\n",
    "            val_accuracy += torch.sum(torch.argmax(pred, dim=1) == targets).item()\n",
    "        size += targets.shape[0]\n",
    "    \n",
    "    val_loss, val_accuracy = val_loss / size, val_accuracy / size\n",
    "    \n",
    "    print(f'Average Validation Loss: {val_loss:.4f} \\t Validation Accuracy: {100*val_accuracy:.2f}%')\n",
    "    \n",
    "    wandb.log({\"train_ce_loss\": train_loss,\n",
    "               \"val_ce_loss\": val_loss,\n",
    "               \"train_accuracy\": 100*train_accuracy,\n",
    "               \"val_accuracy\": 100*val_accuracy,\n",
    "               \"epoch\": epoch\n",
    "               }\n",
    "    )\n",
    "    \n",
    "    vanilla_model.is_training = True\n",
    "\n",
    "vanilla_model.is_training = False\n",
    "test_loss, test_accuracy, size = 0, 0, 0\n",
    "\n",
    "for i, (data, targets) in enumerate(test_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = vanilla_model(data)\n",
    "            test_loss += nn.CrossEntropyLoss()(pred, targets) * targets.shape[0]\n",
    "            test_accuracy += torch.sum(torch.argmax(pred, dim=1) == targets).item()\n",
    "        size += targets.shape[0]\n",
    "\n",
    "test_loss, test_accuracy = test_loss / size, test_accuracy / size\n",
    "\n",
    "print(f'Training finished.')\n",
    "print(f'Average Validation Loss: {test_loss:.4f}\\t Validation Accuracy: {100*test_accuracy:.2f}%')\n",
    "\n",
    "wandb.finish()"
   ],
   "id": "3e08b44480c71b08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student_model = StudentNetwork().to(device)\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=0.01, momentum=0.5, weight_decay=5e-4)\n",
    "num_epochs = 10\n",
    "temperature = 3\n",
    "beta = 0.85\n",
    "alpha = 1\n",
    "\n",
    "wandb.init(\n",
    "    project=\"Renyi_Divergence_MNIST\",\n",
    "    name = \"Student Model\",\n",
    "    config={\n",
    "        \"beta\": beta,\n",
    "        \"temperature\": temperature,\n",
    "        \"alpha\": alpha\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'---Epoch {epoch+1}---')\n",
    "    \n",
    "    train_loss, train_accuracy, size, train_CE_loss = 0, 0, 0, 0\n",
    "    for i, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = student_model(data)\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(data)\n",
    "        loss = (1-beta) * nn.CrossEntropyLoss()(outputs, targets) + beta * RenyiDivergence(alpha=alpha)(outputs/temperature, teacher_outputs/temperature) * (temperature**2) / alpha\n",
    "        CE_loss = nn.CrossEntropyLoss()(outputs, targets)     \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * targets.size(0)\n",
    "        train_CE_loss += CE_loss.item() * targets.size(0)\n",
    "        train_accuracy += torch.sum(torch.argmax(outputs, dim=1) == targets).item()\n",
    "        size += targets.size(0)\n",
    "        \n",
    "    train_loss = train_loss/size\n",
    "    train_CE_loss = train_CE_loss/size\n",
    "    train_accuracy = train_accuracy/size\n",
    "    \n",
    "    print(f'Average Train Loss: {train_loss:.4f} \\t \\t Average Train CE Loss: {train_CE_loss:.4f} \\t \\t Train Accuracy: {100*train_accuracy:.2f}%')\n",
    "    \n",
    "    student_model.is_training = False\n",
    "    \n",
    "    val_loss, val_accuracy, size, val_CE_loss = 0, 0, 0, 0\n",
    "    for i, (data, targets) in enumerate(val_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = student_model(data)\n",
    "            teacher_pred = teacher_model(data)\n",
    "            val_loss += ((1-beta) * nn.CrossEntropyLoss()(pred, targets) + beta * RenyiDivergence(alpha=alpha)(pred/temperature, teacher_pred/temperature) * (temperature**2) / alpha) * targets.shape[0]\n",
    "            val_CE_loss += nn.CrossEntropyLoss()(pred, targets) * targets.shape[0]\n",
    "            val_accuracy += torch.sum(torch.argmax(pred, dim=1) == targets).item()\n",
    "        size += targets.shape[0]\n",
    "    \n",
    "    val_loss, val_accuracy, val_CE_loss = val_loss / size, val_accuracy / size, val_CE_loss/size\n",
    "    \n",
    "    print(f'Average Validation Loss: {val_loss:.4f} \\t Average Validation CE Loss: {val_CE_loss:.4f} \\t Validation Accuracy: {100*val_accuracy:.2f}%')\n",
    "    \n",
    "    wandb.log({\"train_loss\": train_loss,\n",
    "               \"train_ce_loss\": train_CE_loss,\n",
    "               \"val_loss\": val_loss,\n",
    "               \"val_ce_loss\": val_CE_loss,\n",
    "               \"train_accuracy\": 100*train_accuracy,\n",
    "               \"val_accuracy\": 100*val_accuracy,\n",
    "               \"epoch\": epoch\n",
    "               }\n",
    "    )\n",
    "    \n",
    "    student_model.is_training = True\n",
    "\n",
    "student_model.is_training = False\n",
    "test_loss, test_accuracy, size = 0, 0, 0\n",
    "\n",
    "for i, (data, targets) in enumerate(test_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = student_model(data)\n",
    "            test_loss += nn.CrossEntropyLoss()(pred, targets) * targets.shape[0]\n",
    "            test_accuracy += torch.sum(torch.argmax(pred, dim=1) == targets).item()\n",
    "        size += targets.shape[0]\n",
    "\n",
    "test_loss, test_accuracy = test_loss / size, test_accuracy / size\n",
    "\n",
    "print(f'Training finished.')\n",
    "print(f'Average Validation Loss: {test_loss:.4f}\\t Validation Accuracy: {100*test_accuracy:.2f}%')\n",
    "\n",
    "wandb.finish()"
   ],
   "id": "aebbdc1fe0b07860",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student_model = StudentNetwork().to(device)\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=0.01, momentum=0.5, weight_decay=5e-4)\n",
    "num_epochs = 10\n",
    "temperature = 3\n",
    "beta = 0.85\n",
    "alpha = 1"
   ],
   "id": "33a1a59ee7dd049",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(config=None,lr=None,momentum=None,weight_decay=None,temperature=None,beta=None,alpha=None):\n",
    "    with wandb.init(config=config):\n",
    "        global teacher_model\n",
    "        config = wandb.config\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #network = TeacherNetwork().to(device)\n",
    "        network = StudentNetwork().to(device)\n",
    "        #destilation, teacher_model = False, None\n",
    "        destilation, teacher_model = True, teacher_model\n",
    "        epochs = 10\n",
    "        \n",
    "        train_loader, val_loader, test_loader = build_dataset()\n",
    "        \n",
    "        if lr is not None:\n",
    "            config.lr = lr\n",
    "        if momentum is not None:\n",
    "            config.momentum = momentum\n",
    "        if weight_decay is not None:\n",
    "            config.weight_decay = weight_decay\n",
    "        if temperature is not None:\n",
    "            config.temperature = temperature\n",
    "        if beta is not None:\n",
    "            config.beta = beta\n",
    "        if alpha is not None:\n",
    "            config.alpha = alpha\n",
    "        \n",
    "        optimizer = optim.SGD(network.parameters(), lr=config.lr, momentum=config.momentum, weight_decay=config.weight_decay)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_epoch(epoch, network, train_loader, val_loader, optimizer, device, destilation, teacher_model, config)\n",
    "            "
   ],
   "id": "1bb9b032406949cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_dataset():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))])\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [0.95,0.05], generator=torch.Generator().manual_seed(872))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, generator=torch.Generator().manual_seed(872))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True, generator=torch.Generator().manual_seed(872))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "    \n",
    "def train_epoch(epoch, network, train_loader, val_loader, optimizer, device, destilation, teacher_model, config):\n",
    "    train_loss, train_accuracy, size, train_CE_loss = 0, 0, 0, 0\n",
    "    for i, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = network(data)\n",
    "        if destilation:\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(data)\n",
    "            loss = (1-config.beta) * nn.CrossEntropyLoss()(outputs, targets) + config.beta * RenyiDivergence(alpha=config.alpha)(outputs/config.temperature, teacher_outputs/config.temperature) * (config.temperature**2) / config.alpha\n",
    "        else:\n",
    "            loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "        CE_loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * targets.size(0)\n",
    "        train_CE_loss += CE_loss.item() * targets.size(0)\n",
    "        train_accuracy += torch.sum(torch.argmax(outputs, dim=1) == targets).item()\n",
    "        size += targets.size(0)\n",
    "        \n",
    "    train_loss = train_loss/size\n",
    "    train_CE_loss = train_CE_loss/size\n",
    "    train_accuracy = train_accuracy/size\n",
    "    \n",
    "    print(f'Average Train Loss: {train_loss:.4f} \\t \\t Average Train CE Loss: {train_CE_loss:.4f} \\t \\t Train Accuracy: {100*train_accuracy:.2f}%')\n",
    "    \n",
    "    network.is_training = False\n",
    "    \n",
    "    val_loss, val_accuracy, size, val_CE_loss = 0, 0, 0, 0\n",
    "    for i, (data, targets) in enumerate(val_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = network(data)\n",
    "            teacher_pred = teacher_model(data)\n",
    "            val_loss += ((1-config.beta) * nn.CrossEntropyLoss()(pred, targets) + config.beta * RenyiDivergence(alpha=config.alpha)(pred/config.temperature, teacher_pred/config.temperature) * (config.temperature**2) / config.alpha) * targets.shape[0]\n",
    "            val_CE_loss += nn.CrossEntropyLoss()(pred, targets) * targets.shape[0]\n",
    "            val_accuracy += torch.sum(torch.argmax(pred, dim=1) == targets).item()\n",
    "        size += targets.shape[0]\n",
    "    \n",
    "    val_loss, val_accuracy, val_CE_loss = val_loss / size, val_accuracy / size, val_CE_loss/size\n",
    "    \n",
    "    print(f'Average Validation Loss: {val_loss:.4f} \\t Average Validation CE Loss: {val_CE_loss:.4f} \\t Validation Accuracy: {100*val_accuracy:.2f}%')\n",
    "    \n",
    "    wandb.log({\"train_loss\": train_loss,\n",
    "               \"train_ce_loss\": train_CE_loss,\n",
    "               \"val_loss\": val_loss,\n",
    "               \"val_ce_loss\": val_CE_loss,\n",
    "               \"train_accuracy\": 100*train_accuracy,\n",
    "               \"val_accuracy\": 100*val_accuracy,\n",
    "               \"epoch\": epoch\n",
    "               }\n",
    "    )\n",
    "    \n",
    "    network.is_training = True"
   ],
   "id": "198b85b4ff848c35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'beta': {'distribution': 'uniform', 'min': 0.5, 'max': 1},\n",
    "        'temperature': {'distribution': 'int_uniform', 'min': 1, 'max': 25}\n",
    "    }\n",
    "}"
   ],
   "id": "ad9d14df25d6da2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sweep_id = wandb.sweep(sweep_config, project=\"Renyi_Divergence_Sweep_Student\")",
   "id": "74f25816be43ff75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "wandb.agent(sweep_id, function=lambda: train(lr=0.01,momentum=0.5,weight_decay=5e-4,alpha=1), count=30)",
   "id": "9888b96a923b23a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "wandb.finish()",
   "id": "f097539581877885",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b79cf8d43c1cd018",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "73a2e9e143e55212",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e9d6baafe205cf96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    student_model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = student_model(data[0].to(device))\n",
    "        teacher_model_output = teacher_model(data[0].to(device))\n",
    "    break"
   ],
   "id": "f793aab98ccc874b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.bar(np.arange(10), np.squeeze(output.cpu().numpy()), width=0.35, color='blue', label='Student Model Logits')\n",
    "plt.bar(np.arange(10)+0.35, np.squeeze(teacher_model_output.cpu().numpy()), width=0.35, color='green', label='Teacher Model Logits')\n",
    "plt.legend()\n",
    "plt.title(\"Logits\")\n",
    "plt.show()"
   ],
   "id": "3c26656e86577298"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.bar(np.arange(10), np.squeeze(torch.softmax(output,dim=1).cpu().numpy()), width=0.35, color='blue', label='Student Model Logits')\n",
    "plt.bar(np.arange(10)+0.35, np.squeeze(torch.softmax(teacher_model_output,dim=1).cpu().numpy()), width=0.35, color='green', label='Teacher Model Logits')\n",
    "plt.legend()\n",
    "plt.title(\"Sotmax without temperature\")\n",
    "plt.show()"
   ],
   "id": "8a3889f13e507ccd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "temperature = 5\n",
    "\n",
    "plt.bar(np.arange(10), np.squeeze(torch.softmax(output/temperature,dim=1).cpu().numpy()), width=0.35, color='blue', label='Student Model Logits')\n",
    "plt.bar(np.arange(10)+0.35, np.squeeze(torch.softmax(teacher_model_output/temperature,dim=1).cpu().numpy()), width=0.35, color='green', label='Teacher Model Logits')\n",
    "plt.legend()\n",
    "plt.title(\"Sotmax with temperature\")\n",
    "plt.show()"
   ],
   "id": "4f1a4a772dd7dbf8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "52319566f262e453",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5ee8875ecfe3a82e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a5dadfa4f11f0fd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "280f44431061664",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3805be82372b1826",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
